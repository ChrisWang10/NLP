# Attention

## 1. What is attention

Attention is one **component** of a network's architecture, there are two kinds of attention mechanism

a.	General Attention. Between input and output elements

b.	Self-attention. Within the input elements.









